{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:01.375346Z","iopub.status.busy":"2023-05-15T09:43:01.374683Z","iopub.status.idle":"2023-05-15T09:43:10.679904Z","shell.execute_reply":"2023-05-15T09:43:10.678884Z","shell.execute_reply.started":"2023-05-15T09:43:01.375303Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle as pkl\n","import string\n","import random\n","from tqdm import tqdm\n","from PIL import Image \n","\n","# Keras Libraries\n","import tensorflow\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import Input, layers\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import add\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n","from nltk.translate.bleu_score import sentence_bleu"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:10.682458Z","iopub.status.busy":"2023-05-15T09:43:10.681840Z","iopub.status.idle":"2023-05-15T09:43:10.716041Z","shell.execute_reply":"2023-05-15T09:43:10.715225Z","shell.execute_reply.started":"2023-05-15T09:43:10.682430Z"},"trusted":true},"outputs":[],"source":["# Import train,test,val image names from given datafiles  \n","\n","train_image_names = open('/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt','r').read().splitlines()\n","val_image_names = open('/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr_8k.valImages.txt','r').read().splitlines()\n","test_image_names = open('/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt','r').read().splitlines()\n","images_path = '/kaggle/input/flickr/Flickr8k/Flicker8k_Images/'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:10.717656Z","iopub.status.busy":"2023-05-15T09:43:10.717296Z","iopub.status.idle":"2023-05-15T09:43:10.801520Z","shell.execute_reply":"2023-05-15T09:43:10.800406Z","shell.execute_reply.started":"2023-05-15T09:43:10.717614Z"},"trusted":true},"outputs":[],"source":["# Import Lemmatized text descriptions\n","lemma_desc_list  = open('/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr8k.token.txt','r').read().splitlines()"]},{"cell_type":"markdown","metadata":{},"source":["### Cleaning of text descriptions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:10.806312Z","iopub.status.busy":"2023-05-15T09:43:10.805694Z","iopub.status.idle":"2023-05-15T09:43:10.812920Z","shell.execute_reply":"2023-05-15T09:43:10.811884Z","shell.execute_reply.started":"2023-05-15T09:43:10.806271Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(line):\n","    line = line.split()                             # Convert to a list of words\n","    line = [w.lower() for w in line]                # Convert to lowercase\n","    line = [w for w in line if w.isalpha()]         # Remove numbers\n","    line = \" \".join(line).translate(\n","        str.maketrans(\"\", \"\", string.punctuation)   # Remove punctuation\n","    )\n","    line = \"startseq \" + line + \" endseq\"\n","    return line"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:10.815173Z","iopub.status.busy":"2023-05-15T09:43:10.814713Z","iopub.status.idle":"2023-05-15T09:43:11.276619Z","shell.execute_reply":"2023-05-15T09:43:11.275695Z","shell.execute_reply.started":"2023-05-15T09:43:10.815136Z"},"trusted":true},"outputs":[],"source":["# Storing the descriptions in a dictionary\n","lemmatized_text_desc = {}\n","for i in lemma_desc_list:\n","    image_name = i.split('\\t')[0]\n","    image_name = image_name.split('#')[0]\n","    text = i.split('\\t')[1]\n","    text = preprocess_text(text)\n","    if image_name in lemmatized_text_desc:\n","        lemmatized_text_desc[image_name].append(text)\n","    else:\n","        lemmatized_text_desc[image_name] = [text]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:11.278515Z","iopub.status.busy":"2023-05-15T09:43:11.278173Z","iopub.status.idle":"2023-05-15T09:43:11.289361Z","shell.execute_reply":"2023-05-15T09:43:11.288045Z","shell.execute_reply.started":"2023-05-15T09:43:11.278482Z"},"trusted":true},"outputs":[],"source":["# Split into train, test and val descriptors for our model\n","train_text = {}\n","val_text = {}\n","test_text = {}\n","for i in train_image_names:\n","    train_text[i] = lemmatized_text_desc[i]\n","for i in val_image_names:\n","    val_text[i] = lemmatized_text_desc[i]\n","for i in test_image_names:\n","    test_text[i] = lemmatized_text_desc[i]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:11.291495Z","iopub.status.busy":"2023-05-15T09:43:11.290899Z","iopub.status.idle":"2023-05-15T09:43:11.336997Z","shell.execute_reply":"2023-05-15T09:43:11.336179Z","shell.execute_reply.started":"2023-05-15T09:43:11.291461Z"},"trusted":true},"outputs":[],"source":["max_length = 0\n","for filename,texts in lemmatized_text_desc.items():\n","    for i in texts:\n","        if(max_length < len(i.split())):\n","            max_length = len(i.split())\n","            max_string = i\n","            max_list = i.split()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:11.338345Z","iopub.status.busy":"2023-05-15T09:43:11.338025Z","iopub.status.idle":"2023-05-15T09:43:11.344452Z","shell.execute_reply":"2023-05-15T09:43:11.343524Z","shell.execute_reply.started":"2023-05-15T09:43:11.338314Z"},"trusted":true},"outputs":[{"data":{"text/plain":["36"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["max_length"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:11.346492Z","iopub.status.busy":"2023-05-15T09:43:11.345666Z","iopub.status.idle":"2023-05-15T09:43:11.529741Z","shell.execute_reply":"2023-05-15T09:43:11.528795Z","shell.execute_reply.started":"2023-05-15T09:43:11.346460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["7276\n"]}],"source":["word_counts = {}\n","nsents = 0\n","for key,values in train_text.items():\n","    for i in values:\n","        nsents += 1\n","        for w in i.split(' '):\n","            word_counts[w] = word_counts.get(w, 0) + 1\n","vocabulary = [w for w in word_counts]\n","print(len(vocabulary))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:11.533694Z","iopub.status.busy":"2023-05-15T09:43:11.533417Z","iopub.status.idle":"2023-05-15T09:43:13.470363Z","shell.execute_reply":"2023-05-15T09:43:13.469341Z","shell.execute_reply.started":"2023-05-15T09:43:11.533670Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["7276\n"]}],"source":["vocabulary1 = []\n","\n","for key,values in train_text.items():\n","    for i in values:\n","        for w in i.split(' '):\n","            if(w not in vocabulary1): vocabulary1.append(w)\n","print(len(vocabulary1))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:13.472518Z","iopub.status.busy":"2023-05-15T09:43:13.471901Z","iopub.status.idle":"2023-05-15T09:43:24.987593Z","shell.execute_reply":"2023-05-15T09:43:24.986565Z","shell.execute_reply.started":"2023-05-15T09:43:13.472481Z"},"trusted":true},"outputs":[],"source":["# Read GloVe files\n","\n","with open(\"/kaggle/input/glove6b200d/glove.6B.200d.txt\", \"r\") as f:\n","    glove = f.read().split(\"\\n\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:24.998531Z","iopub.status.busy":"2023-05-15T09:43:24.997729Z","iopub.status.idle":"2023-05-15T09:43:51.125931Z","shell.execute_reply":"2023-05-15T09:43:51.124966Z","shell.execute_reply.started":"2023-05-15T09:43:24.998496Z"},"trusted":true},"outputs":[],"source":["# Initialize the dictionary\n","glove_dict = {}\n","\n","for line in glove:\n","    try:\n","        elements = line.split()\n","        word, vector = elements[0], np.array([float(i) for i in elements[1:]])\n","        glove_dict[word] = vector\n","    except:\n","        continue"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:43:51.128080Z","iopub.status.busy":"2023-05-15T09:43:51.127676Z","iopub.status.idle":"2023-05-15T09:44:11.000848Z","shell.execute_reply":"2023-05-15T09:44:10.999878Z","shell.execute_reply.started":"2023-05-15T09:43:51.128043Z"},"trusted":true},"outputs":[],"source":["embeddings_index = {} \n","glove_file = open('/kaggle/input/glove6b200d/glove.6B.200d.txt', encoding=\"utf-8\")\n","for line in glove_file:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:44:11.002781Z","iopub.status.busy":"2023-05-15T09:44:11.002386Z","iopub.status.idle":"2023-05-15T09:44:11.026015Z","shell.execute_reply":"2023-05-15T09:44:11.025117Z","shell.execute_reply.started":"2023-05-15T09:44:11.002745Z"},"trusted":true},"outputs":[],"source":["ixtoword = {}\n","wordtoix = {}\n","ix = 1\n","for w in vocabulary:             \n","    wordtoix[w] = ix\n","    ixtoword[ix] = w\n","    ix += 1\n","\n","vocab_size = len(ixtoword) + 1"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:44:11.028070Z","iopub.status.busy":"2023-05-15T09:44:11.027443Z","iopub.status.idle":"2023-05-15T09:44:11.042941Z","shell.execute_reply":"2023-05-15T09:44:11.041983Z","shell.execute_reply.started":"2023-05-15T09:44:11.028031Z"},"trusted":true},"outputs":[],"source":["ixtoword1 = {}\n","wordtoix1 = {}\n","ix = 1\n","for w in vocabulary1:\n","    wordtoix1[w] = ix\n","    ixtoword1[ix] = w\n","    ix += 1\n","\n","vocab_size1 = len(ixtoword1) + 1"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:44:11.095468Z","iopub.status.busy":"2023-05-15T09:44:11.094622Z","iopub.status.idle":"2023-05-15T09:44:11.132676Z","shell.execute_reply":"2023-05-15T09:44:11.131871Z","shell.execute_reply.started":"2023-05-15T09:44:11.095370Z"},"trusted":true},"outputs":[],"source":["glove_weights1 = np.random.uniform(0, 1, (vocab_size1, 200))\n","for word, i in wordtoix.items():\n","    embedding_vector = glove_dict.get(word)\n","    if embedding_vector is not None:\n","        glove_weights1[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CNN model (ResNet50) for feature extraction from images\n","\n","resnet_model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n","resnet_model.summary()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:44:11.146670Z","iopub.status.busy":"2023-05-15T09:44:11.145531Z","iopub.status.idle":"2023-05-15T09:44:15.190414Z","shell.execute_reply":"2023-05-15T09:44:15.189407Z","shell.execute_reply.started":"2023-05-15T09:44:11.146633Z"},"trusted":true},"outputs":[],"source":["from torchvision import transforms\n","\n","img_transform = transforms.Compose([transforms.Resize((224, 224))])"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T09:44:15.193852Z","iopub.status.busy":"2023-05-15T09:44:15.192714Z","iopub.status.idle":"2023-05-15T09:44:15.199592Z","shell.execute_reply":"2023-05-15T09:44:15.198658Z","shell.execute_reply.started":"2023-05-15T09:44:15.193795Z"},"trusted":true},"outputs":[],"source":["def img_preprocess(img_path):\n","    im = cv2.imread(images_path  + img_path)   \n","    im_res = cv2.resize(im,(224,224))\n","    im_res = np.expand_dims(im_res, axis=0)\n","    return im_res"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Predict the feature vectors for each training image \n","\n","# train_data = {}\n","# ctr=0\n","# for ix in train_image_names:\n","#     if ix == \"\":\n","#         continue\n","#     ctr+=1\n","#     if ctr%500==0:\n","#         print(ctr)\n","#     path = ix\n","#     img = img_preprocess(path)\n","#     pred = resnet_model.predict(img).reshape(2048)\n","#     train_data[ix] = pred"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Predict the feature vectors for each training image \n","\n","# train_data = {}\n","# ctr=0\n","# for ix in train_image_names:\n","#     if ix == \"\":\n","#         continue\n","#     ctr+=1\n","#     if ctr%500==0:\n","#         print(ctr)\n","#     path = ix\n","#     img = Image.open(images_path + path)\n","#     img = img_transform(img)\n","#     # Add a batch dimension to the tensor\n","#     img = np.expand_dims(img, axis=0)\n","#     pred = resnet_model.predict(img).reshape(2048)\n","#     train_data[ix] = pred\n","#     # print(img.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# filename = 'cnn_train_features1.pickle'\n","# file = open(filename, 'wb')\n","# pkl.dump(train_data,file)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:09:07.554149Z","iopub.status.busy":"2023-05-15T10:09:07.553450Z","iopub.status.idle":"2023-05-15T10:09:07.623016Z","shell.execute_reply":"2023-05-15T10:09:07.622067Z","shell.execute_reply.started":"2023-05-15T10:09:07.554114Z"},"trusted":true},"outputs":[],"source":["filename = '/kaggle/input/features1/cnn_train_features1.pickle'\n","file = open(filename, 'rb')\n","trainImg_features = pkl.load(file)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:09:08.493293Z","iopub.status.busy":"2023-05-15T10:09:08.492346Z","iopub.status.idle":"2023-05-15T10:09:09.469082Z","shell.execute_reply":"2023-05-15T10:09:09.468333Z","shell.execute_reply.started":"2023-05-15T10:09:08.493247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 2048)]       0           []                               \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 1, 2048)      0           ['input_3[0][0]']                \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 1, 2048)      0           ['reshape_3[0][0]']              \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 36)]         0           []                               \n","                                                                                                  \n"," dense_7 (Dense)                (None, 1, 200)       409800      ['dropout_4[0][0]']              \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 36, 200)      1455400     ['input_4[0][0]']                \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 1, 200)       0           ['dense_7[0][0]']                \n","                                                                                                  \n"," dense_8 (Dense)                (None, 36, 200)      40200       ['embedding_1[0][0]']            \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 37, 200)      0           ['reshape_4[0][0]',              \n","                                                                  'dense_8[0][0]']                \n","                                                                                                  \n"," dense_9 (Dense)                (None, 37, 200)      40200       ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dense_10 (Dense)               (None, 37, 200)      40200       ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dense_11 (Dense)               (None, 37, 200)      40200       ['concatenate_1[0][0]']          \n","                                                                                                  \n"," attention_1 (Attention)        (None, 37, 200)      0           ['dense_9[0][0]',                \n","                                                                  'dense_10[0][0]',               \n","                                                                  'dense_11[0][0]']               \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 37, 200)      0           ['attention_1[0][0]']            \n","                                                                                                  \n"," lstm_3 (LSTM)                  (None, 37, 200)      320800      ['dropout_5[0][0]']              \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 37, 200)      0           ['lstm_3[0][0]']                 \n","                                                                                                  \n"," lstm_4 (LSTM)                  (None, 37, 200)      320800      ['dropout_6[0][0]']              \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 37, 200)      0           ['lstm_4[0][0]']                 \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 200)          0           ['dense_7[0][0]']                \n","                                                                                                  \n"," lstm_5 (LSTM)                  (None, 200)          320800      ['dropout_7[0][0]']              \n","                                                                                                  \n"," add_1 (Add)                    (None, 200)          0           ['reshape_5[0][0]',              \n","                                                                  'lstm_5[0][0]']                 \n","                                                                                                  \n"," dense_12 (Dense)               (None, 200)          40200       ['add_1[0][0]']                  \n","                                                                                                  \n"," dense_13 (Dense)               (None, 7277)         1462677     ['dense_12[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 4,491,277\n","Trainable params: 4,491,277\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["from tensorflow.keras.layers import concatenate, LSTM, Attention, MultiHeadAttention, LayerNormalization\n","\n","# #Baseline- wihout attention\n","# inputs1 = Input(shape=(2048,))\n","# inputs2 = Input(shape=(max_length,))\n","\n","# fe1 = Dropout(0.5)(inputs1)\n","# fe2 = Dense(200, activation='relu')(fe1)\n","# fe3 = Reshape((1, 200), input_shape=(200,))(fe2)\n","\n","# se1 = Embedding(vocab_size, 200, mask_zero=False)(inputs2)\n","# merged = concatenate([fe3, se1], axis = 1)\n","# se2 = LSTM(200, return_sequences = True)(merged)\n","# se3 = Dropout(0.5)(se2)\n","\n","# decoder1 = add([fe2, se3])\n","# decoder2 = Dense(200, activation='relu')(decoder1)\n","# outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","# model.summary()\n","\n","# # Attention-1\n","inputs1 = Input(shape=(2048,))\n","in1 = Reshape((1, 2048), input_shape=(2048,))(inputs1)\n","\n","feat_l1 = Dropout(0.5)(in1)\n","feat_l2 = Dense(200, activation = 'relu')(feat_l1)\n","fe3 = Reshape((1, 200), input_shape=(200,))(feat_l2)\n","\n","\n","# sequence input -> second path\n","in2 = Input(shape=(max_length,))\n","emb = Embedding(vocab_size, 200, weights=[glove_weights1], trainable=True, mask_zero=False)(in2)\n","emb = Dense(200, activation = 'relu')(emb)\n","\n","comb_l1 = concatenate([fe3, emb], axis = 1)\n","query = Dense(200, activation = 'relu')(comb_l1)\n","value = Dense(200, activation = 'relu')(comb_l1)\n","key = Dense(200, activation = 'relu')(comb_l1)\n","atte_layer1 = Attention()([query, value, key])\n","\n","seq_l1 = Dropout(0.1)(atte_layer1)\n","seq_l2 = LSTM(200, return_sequences = True)(seq_l1)\n","\n","seq_l3 = Dropout(0.1)(seq_l2)\n","seq_l4 = LSTM(200, return_sequences = True)(seq_l3)\n","\n","seq_l5 = Dropout(0.1)(seq_l4)\n","seq_l6 = LSTM(200)(seq_l5)\n","\n","\n","comb_l2 = add([Reshape((200, ))(feat_l2), seq_l6])\n","comb_l3 = Dense(200, activation = 'relu')(comb_l2)\n","\n","# output\n","output = Dense(vocab_size, activation = 'softmax')(comb_l3)\n","\n","# compile model\n","model = Model(inputs = [inputs1, in2], outputs = output)\n","model.summary()\n","\n","\n","\n","# # MultiHead Attention\n","# # Define input layers\n","# inputs1 = Input(shape=(2048,))\n","# inputs2 = Input(shape=(max_length,))\n","\n","# # Define the feature extractor network\n","# fe1 = Dropout(0.5)(inputs1)\n","# fe2 = Dense(200, activation='relu')(fe1)\n","# fe3 = Reshape((1, 200), input_shape=(200,))(fe2)\n","\n","# # Define the sequence encoder network with attention\n","# se1 = Embedding(vocab_size, 200, mask_zero=False)(inputs2)\n","# se2 = LSTM(200, return_sequences=True)(se1)\n","# attn = MultiHeadAttention(num_heads=8, key_dim=4)(fe3, se2)\n","# context = concatenate([attn, fe3], axis=-1)\n","# se3 = LSTM(200)(context)\n","# se4 = Dropout(0.5)(se3)\n","\n","# # Define the decoder network\n","# decoder1 = add([fe2, se4])\n","# decoder2 = Dense(200, activation='relu')(decoder1)\n","# outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","# # Define the model\n","# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","# model.summary()\n","\n","# # MultiheadAttention-2\n","# inputs1 = Input(shape=(2048,))\n","# inputs2 = Input(shape=(max_length,))\n","\n","# fe1 = Dropout(0.5)(inputs1)\n","# fe2 = Dense(200, activation='relu')(fe1)\n","# fe3 = Reshape((1, 200), input_shape=(200,))(fe2)\n","\n","# se1 = Embedding(vocab_size, 200, mask_zero=False)(inputs2)\n","# se2 = LSTM(200, return_sequences=True)(se1)\n","\n","# # Adding Multi-Head Attention layer\n","# attn = MultiHeadAttention(num_heads=8, key_dim=64)(se2, se2, se2)\n","# attn = Dropout(0.5)(attn)\n","# attn = LayerNormalization(epsilon=1e-6)(attn)\n","\n","# # Concatenating the context vector and features from CNN\n","# context = concatenate([attn, fe3], axis=1)\n","# se3 = LSTM(200)(context)\n","# se4 = Dropout(0.5)(se3)\n","\n","# decoder1 = add([fe2, se4])\n","# decoder2 = Dense(200, activation='relu')(decoder1)\n","# outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","# model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Setting the embedding layer weights to the weights we predicted from the word embeddings\n","model.layers[5].set_weights([glove_weights1])\n","model.layers[5].trainable = True"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:09:13.762443Z","iopub.status.busy":"2023-05-15T10:09:13.762078Z","iopub.status.idle":"2023-05-15T10:09:13.770565Z","shell.execute_reply":"2023-05-15T10:09:13.769644Z","shell.execute_reply.started":"2023-05-15T10:09:13.762413Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","optimizer = Adam(lr = 0.0001)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:09:19.639263Z","iopub.status.busy":"2023-05-15T10:09:19.638908Z","iopub.status.idle":"2023-05-15T10:09:19.652652Z","shell.execute_reply":"2023-05-15T10:09:19.651694Z","shell.execute_reply.started":"2023-05-15T10:09:19.639235Z"},"trusted":true},"outputs":[],"source":["model.compile(loss='categorical_crossentropy', optimizer=optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:09:20.736663Z","iopub.status.busy":"2023-05-15T10:09:20.736310Z","iopub.status.idle":"2023-05-15T10:09:20.751541Z","shell.execute_reply":"2023-05-15T10:09:20.750558Z","shell.execute_reply.started":"2023-05-15T10:09:20.736634Z"},"trusted":true},"outputs":[],"source":["# We will create batches and pass it for training\n","def data_loader(descs, imgs, wrd_to_indx, max_len, batch_size):\n","    X1, X2, Y = [], [], []\n","    n = 0\n","\n","    while True:\n","        for img_name, desc in descs.items():\n","            # n += 1\n","\n","            img = imgs[img_name]\n","            for d in desc:\n","                # Encoding of the sentence\n","                seq = [wrd_to_indx[word] for word in d.split(' ') if word in wrd_to_indx]\n","                for i in range(1, len(seq)):\n","                    # split into input and output pair\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_len)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size1)[0]\n","                    # store\n","                    X1.append(img)\n","                    X2.append(in_seq)\n","                    Y.append(out_seq)\n","                    \n","            n += 1\n","\n","            if n==batch_size:\n","                yield ([np.array(X1), np.array(X2)], np.array(Y))\n","                X1, X2, Y = [], [], []\n","                n=0"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:09:21.060551Z","iopub.status.busy":"2023-05-15T10:09:21.060201Z","iopub.status.idle":"2023-05-15T10:26:54.194866Z","shell.execute_reply":"2023-05-15T10:26:54.193869Z","shell.execute_reply.started":"2023-05-15T10:09:21.060522Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","2000/2000 [==============================] - 88s 40ms/step - loss: 4.6354\n","Epoch 2/15\n","2000/2000 [==============================] - 69s 35ms/step - loss: 4.0198\n","Epoch 3/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.8744\n","Epoch 4/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.8411\n","Epoch 5/15\n","2000/2000 [==============================] - 69s 35ms/step - loss: 3.6825\n","Epoch 6/15\n","2000/2000 [==============================] - 69s 35ms/step - loss: 3.6957\n","Epoch 7/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.4787\n","Epoch 8/15\n","2000/2000 [==============================] - 69s 35ms/step - loss: 3.4498\n","Epoch 9/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.3580\n","Epoch 10/15\n","2000/2000 [==============================] - 69s 35ms/step - loss: 3.2951\n","Epoch 11/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.2898\n","Epoch 12/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.2608\n","Epoch 13/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.2008\n","Epoch 14/15\n","2000/2000 [==============================] - 68s 34ms/step - loss: 3.2774\n","Epoch 15/15\n","2000/2000 [==============================] - 69s 34ms/step - loss: 3.2696\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x79809024d330>"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["epochs = 15 #15\n","batch_size = 3 #3\n","steps = len(train_text)//batch_size\n","\n","generator = data_loader(train_text, trainImg_features, wordtoix1, max_length, batch_size)\n","\n","# with tensorflow.device('gpu'):\n","model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Removes start and end seq from test captions\n","def remove_seq(test_captions):\n","    for i in range(len(test_captions)):\n","        text = test_captions[i]\n","        word_list = text.split()\n","        word_list = word_list[1:-1]\n","        test_captions[i] = ' '.join(word_list)\n","    return test_captions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict_caption(photo):\n","    in_text = 'startseq'\n","    for i in range(max_length):\n","        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        \n","        yhat = model.predict([photo,sequence], verbose=0)\n","        \n","        yhat = np.argmax(yhat)\n","        word = ixtoword[yhat]\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","\n","    final = in_text.split()\n","    final = final[1:-1]\n","    final = ' '.join(final)\n","    return final"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T10:27:16.050645Z","iopub.status.busy":"2023-05-15T10:27:16.050273Z","iopub.status.idle":"2023-05-15T10:27:16.215743Z","shell.execute_reply":"2023-05-15T10:27:16.214781Z","shell.execute_reply.started":"2023-05-15T10:27:16.050615Z"},"trusted":true},"outputs":[],"source":["model.save('keras_attention_true.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import torch\n","# torch.save(model, 'model_keras.h5' )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import pickle as pkl\n","# filename = 'model_keras1.pickle'\n","# file = open(filename, 'wb')\n","# pkl.dump(model,file)"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_name = test_image_names[23]\n","# img = img_preprocess(image_name)\n","# img = Image.open(images_path + image_name)\n","img = cv2.imread(images_path + image_name) \n","# img = img_transform(img)\n","# img = cv2.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + tr_img)\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","img = cv2.resize(img,(224,224))\n","img = np.expand_dims(img, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with tensorflow.device('gpu'):\n","    pred = resnet_model.predict(img).reshape(1,2048)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = plt.imread(images_path+image_name)\n","plt.imshow(x)\n","plt.show()\n","\n","prediction = predict_caption(pred)\n","print(prediction)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation with BLEU scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_model(img_list):\n","    scores = []\n","    preds = []\n","#     img_list = random.sample(img_list, 100)\n","    for image_name in tqdm(img_list):    \n","        img = img_preprocess(image_name)\n","        pred = resnet_model.predict(img).reshape(1,2048)\n","        \n","        prediction = predict_caption(pred)\n","        preds.append(prediction)\n","        \n","        reference = test_text[image_name].copy()\n","        reference = remove_seq(reference)\n","\n","        score = sentence_bleu(reference, prediction)\n","        scores.append(score)\n","    return scores,preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["g_scores, g_predictions = evaluate_model(test_image_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["np.mean(g_scores)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Meteor Scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.translate.meteor_score import meteor_score\n","\n","scores_list = []\n","test_images_list = test_image_names.copy()\n","#test_images_list = random.sample(test_images_list, 100)\n","\n","for img_name in tqdm(test_images_list):   \n","    predictions_list = []\n","    # img = cv2.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + img_name)\n","    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    # img = cv2.resize(img,(224,224))\n","    # img = np.expand_dims(img, axis=0)\n","    img = img_preprocess(img_name)\n","    # with tensorflow.device('gpu'):\n","    pred = resnet_model.predict(img, verbose = 0).reshape(1,2048)\n","\n","    pred = algo(pred)\n","    predictions_list.append(pred)\n","\n","    reference = test_text[img_name].copy()\n","    reference = remove_seq(reference)\n","\n","\n","    pred_words = pred.split()\n","    print(reference)\n","    score = meteor_score([x.split() for x in reference], pred.split())\n","    scores_list.append(score)   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(scores_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu\n","\n","references =[]\n","for image_name in tqdm(test_image_names):    \n","    reference = test_text[image_name].copy()\n","    reference = remove_seq(reference)\n","    word_list = []    \n","    for sen in reference:\n","        word_list.append(sen.split())\n","    references.append(word_list)\n","\n","predictions_words = []\n","for prediction in g_predictions:\n","    predictions_words.append(prediction.split())\n","\n","bleu_1 = corpus_bleu(references, predictions_words, weights=(1.0, 0, 0, 0))\n","bleu_2 = corpus_bleu(references, predictions_words, weights=(0.5, 0.5, 0, 0))\n","bleu_3 = corpus_bleu(references, predictions_words, weights=(0.33, 0.33, 0.33, 0))\n","bleu_4 = corpus_bleu(references, predictions_words, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","print(\"BLEU-1: {:.4f}\".format(bleu_1))\n","print(\"BLEU-2: {:.4f}\".format(bleu_2))\n","print(\"BLEU-3: {:.4f}\".format(bleu_3))\n","print(\"BLEU-4: {:.4f}\".format(bleu_4))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def predict_beam_search(image, beam_index = 3):\n","#     start = [wordtoix[\"startseq\"]]\n","#     start_word = [[start, 0.0]]\n","#     while len(start_word[0][0]) < max_length:\n","#         temp = []\n","#         for s in start_word:\n","#             par_caps = pad_sequences([s[0]], maxlen=max_length, padding='post')\n","#             preds = model.predict([image,par_caps], verbose=0)\n","#             word_preds = np.argsort(preds[0])[-beam_index:]\n","#             # Getting the top <beam_index>(n) predictions and creating a \n","#             # new list so as to put them via the model again\n","#             for w in word_preds:\n","#                 next_cap, prob = s[0][:], s[1]\n","#                 next_cap.append(w)\n","#                 prob += preds[0][w]\n","#                 temp.append([next_cap, prob])\n","                    \n","#         start_word = temp\n","#         # Sorting according to the probabilities\n","#         start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n","#         # Getting the top words\n","#         start_word = start_word[-beam_index:]\n","    \n","#     start_word = start_word[-1][0]\n","#     intermediate_caption = [ixtoword[i] for i in start_word]\n","#     final_caption = []\n","    \n","#     for i in intermediate_caption:\n","#         if i != 'endseq':\n","#             final_caption.append(i)\n","#         else:\n","#             break\n","\n","#     final_caption = ' '.join(final_caption[1:])\n","#     return final_caption"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
